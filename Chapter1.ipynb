{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "--------------\n",
    "\n",
    "### ニューラルネットとは\n",
    "--------\n",
    "コンピュータに、観測データにもとづいて学習する能力を与える、生物学にヒントを得たプログラミングパラダイム。\n",
    "\n",
    "### 深層学習とは\n",
    "------\n",
    "ニューラルネットワークに学習をさせるための強力な手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章\n",
    "\n",
    "------------------------\n",
    "\n",
    "## パーセプトロンとは\n",
    "\n",
    "- 人工ニューロンの1種\n",
    "- 1950年代から1960年代にかけて開発される\n",
    "- 今日では、パーセプトロン以外の人工ニューロンモデルを扱うことが一般的\n",
    "- 現代のニューラルネットワークの研究の多くは、シグモイドニューロンと呼ばれるモデルが主に使われている\n",
    "\n",
    "### パーセプトロンの仕組みについて\n",
    "\n",
    "![title](image/perceptron.png)\n",
    "\n",
    "- 複数の2進数x1,x2,...を入力にとり、1つの2進数を出力する\n",
    "- それぞれの入力が出力に及ぼす影響の大きさを表す実数\"重み\"(w1,w2,...)を用いて出力を計算\n",
    "- パーセプトロンの出力が0になるか1になるかは、入力の重みつき和$\\sum_j w_j x_j$と閾値の大小比較で決まる\n",
    "- 重みと同じく、閾値もパーセプトロンの挙動を決める実数パラメータ\n",
    "\n",
    "\\begin{equation}\n",
    "    output = \\left\\{ \\begin{array}{ll}\n",
    "        0 \\hspace{15pt}if \\sum_j w_j x_j \\leq threshold \\\\\n",
    "        1 \\hspace{15pt}if \\sum_j w_j x_j > threshold\n",
    "    \\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "- パーセプトロンを複雑に組み合わせることで、複雑な判断も可能になる。\n",
    "\n",
    "--------------------------------\n",
    "ここで、先程の条件式は煩雑なので書き換えてみる. \n",
    "\n",
    "まず、$\\sum_j w_j x_j $ という和は内積を使って、$ w \\cdot x \\equiv \\sum_j w_j x_j $ と書くことができる.  \n",
    "次に、閾値を不等式の左辺に移項し、パーセプトロンのバイアス $b \\equiv -threshold $ と呼ばれる量に置き換える.  \n",
    "すると式は次のようになる.\n",
    "\n",
    "\\begin{equation}\n",
    "    output = \\left\\{ \\begin{array}{ll}\n",
    "        0 \\hspace{15pt}if \\ w \\cdot x + b \\leq 0  \\\\\n",
    "        1 \\hspace{15pt}if \\ w \\cdot x + b  >  0\n",
    "    \\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "バイアスは、パーセプトロンが1を出力する傾向の高さを表す量だとみなすことができる.\n",
    "\n",
    "-------------------\n",
    "ここまでは、パーセプトロンを入力情報に重みをつけて判断を行う手続きとして用いてきたが、論理関数を計算するという別の用途もある.\n",
    "\n",
    "あらゆる計算は、AND, OR, そしてNANDから構成されている、とみなすことができる.パーセプトロンは、こういった論理関数を表現できる.例えば、２つの入力をとり、どちらも重みが-2で、全体のバイアスが3であるようなパーセプトロンを考えてみる.\n",
    "\n",
    "![title](image/nand.png)\n",
    "\n",
    "すると、このパーセプトロンは00を入力されると、$(-2) * 0 + (-2) * 0 + 3 = 3$ は正の数だからである.同じように計算すると、01や10を入力しても1を出力することがわかるが、11を入力した場合だけ0が出力される.ということは、このパーセプトロンは、NANDゲートを実装していることになる.\n",
    "\n",
    "よって、NANDゲートがあればどんな計算も構成することができるので、パーセプトロンのネットワークさえあれば任意の論理関数を計算することができる.\n",
    "\n",
    "以上より、パーセプトロンはどの計算装置にも負けない強力さを持つが、単にNANDゲートの亜種にすぎないとも言える.  ところが、ニューラルネットワークの重みとバイアスを自動的に最適化するような、学習アルゴリズム を開発することができるからである.この最適化は、プログラマの直接介入なしに、外部刺激に反応して勝手に起こる.これのおかげで、人工ニューロンは、従来の理論ゲートとは全く異なった使い方ができるようになった.NANDゲートや他の種類の論理ゲートはすべて手動で配線してやる必要があったのに対し、ニューラルネットワークは問題の解き方を自発的に学習してくれる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## シグモイドニューロン\n",
    "--------------------\n",
    "\n",
    "### ニューラルネットワークに対してアルゴリズムをどう設計するか\n",
    "\n",
    "もし、重みやバイアスを微笑に変化させた場合の出力の変化もまた微小である、という性質が本当に成り立っていれば、その声質を使って、ニューラルネットワークがより自分の思ったとおりの挙動を示すように重みとバイアスを修正できる.\n",
    "\n",
    "![title](image/smallchange.png)\n",
    "\n",
    "「９」であるべき数字を間違って「８」と分類したときに、私たちは重みやバイアスに変化を与えて、どうすればこのニューラルネットワークがこの画像を正しく「９」と分類する方向に近づくかを探ることができる.  \n",
    "この過程を繰り返し、重みとバイアスを変化させ続ければ、生成される結果は次第に改善されていく. このようニューラルネットワークは学習する.\n",
    "\n",
    "問題は、ニューラルネットワークがパーセプトロンで構成されていたとすると、このような学習は怒らないこと. 実際、ニューラルネットワーク内のパーセプトロンのうち、どれか１つの重みやバイアスを少し変えると、そのパーセプトロンの出力は、変化がないか、もしくは0から1へというようにすっかり反転してしまう.このように反転してしまうと、ニューラルネットワークの他の部分の挙動も、連動して複雑に変わっていってしまう.\n",
    "\n",
    "今のところパーセプトロンで構成されたニューラルネットワークに上手に学習させる方法は明らかにはなっていない.\n",
    "\n",
    "### シグモイドニューロンとは\n",
    "\n",
    "前述のような問題は、シグモイドニューロンと呼ばれる人工ニューロンを導入することによって克服することができる.  シグモイドニューロンはパーセプトロンと似ているが、シグモイドニューロンの重みやバイアスに微小な変化を与えたとき、それに応じて生じる出力の変化も微小なものにとどまるように調整されている.これによって、シグモイドニューロンで構成されているニューラルネットワークは学習可能になる.\n",
    "\n",
    "![title](image/perceptron.png)\n",
    "\n",
    "パーセプトロンと同じように、シグモイドニューロンは$x_1, x_2,...$ といった入力をとる.しかし、単に0や1だけでなく、0から1の間のあらゆる値をとることができる.また、重みやバイアスもパーセプトロンと同じように持っている. しかし、出力は、０や１だけではなく、代わりに$ \\sigma(w \\cdot x + b) $ という値をとる. $\\sigma$はシグモイド関数と呼ばれていて、次の式で定義される:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\sigma(z) \\ \\equiv \\frac{1}{1+e^{-z}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "より明確に表現すると、シグモイドニューロンの出力は、入力が$x_1, x_2,...$ で、重みが$w_1, w_2, ...$で、そしてバイアスが$b$のとき、次の形をとる.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\frac{1}{1 \\ + \\ \\exp(- \\ \\Sigma_j w_j x_j \\ - \\ b)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "一見、パーセプトロンとシグモイドニューロンは大きく異るように見えるが、大きく共通点がある.\n",
    "\n",
    "パーセプトロンとの共通点を理解するために、$z \\equiv w \\cdot x + b$を大きな正の数としてみる. このとき、$e^{-z} \\approx 0 $ つまり $\\sigma(z) \\approx 1$となる. 言い換えると、$ z=w \\cdot x + b $が大きな数であるとき、シグモイドニューロンの出力はほぼ1となり、パーセプトロンと同じになる. 逆に、$ z = w \\cdot x + b$を大きな負の数としてみる. そのとき$ e^{-z} \\to \\infty $ であり、$\\sigma(z) \\approx 0 $ となる. つまり、$ z=w \\cdot x + b $が大きな負の数であるときも、シグモイドニューロンはパーセプトロンとほぼ同じ動きをする. ただし、$ w \\cdot x + b $ がそこまで大きな数でない場合は、パーセプトロンと同じにはならない.\n",
    "\n",
    "$\\sigma$について重要なのはどういう形のグラフであるかである.数がそのグラフの形である.\n",
    "\n",
    "![title](image/sigmoidgraph.jpg)\n",
    "\n",
    "このグラフはステップ関数のなめらか版である.\n",
    "\n",
    "![title](image/stepgraph.jpg)\n",
    "\n",
    "もし$\\sigma$がステップ関数であれば、シグモイドニューロンはパーセプトロンと等しくなる. $\\sigma$関数の滑らかさは、重みについて$\\Delta w_j$、バイアスについて$\\Delta b$の小さな変化は、ニューロンの出力について$\\Delta \\mbox{output}$の小さな変化を生み出すということを意味している.実際下記の計算から、$\\Delta \\mbox{output}$は大体うまくいってることがわかる.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\Delta \\mbox{output} \\approx \\Sigma_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "    \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\Delta \\mbox{output}$は重みとバイアスにおいて、$\\Delta w_j$ と $\\Delta b$の変化に対し、線形である、といっているのである. この線形性は、欲しいoutputがどんな小さな変化でも、重みとバイアスを小さく変化させることで簡単に得られることを示している. このことからシグモイドニューロンが、より容易に重みとバイアスの変化がoutputを変化させることがわかる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークのアーキテクチャ\n",
    "------------------\n",
    "手書き数字の分類においてとても上手く働くニューラルネットワークを紹介する.その準備として、いくつかの専門用語を説明するためにニューラルネットワークのそれぞれの部門に名前をつけておく.\n",
    "\n",
    "![title](image/neuralnetwork.png)\n",
    "\n",
    "一番左の層が入力層(input layer)、真ん中のニューロンを入力ニューロン(input neurons)、一番右の層または出力層(output layer)は、出力ニューロン(ouput neuranos)から構成されている. 中央の層は、隠れ層(hidden layer)と呼ばれる. 隠れ層が複数あるような複数層のネットワークをときおり多層パーセプトロン(multilayer perceptrons)、またはMLPsと呼ぶ. しかし、実際はパーセプトロンではなく、シグモイドニューロンである.\n",
    "\n",
    "例えば、手書きの画像が9かそうでないかを判定したいとする. もしその画像が64×64の白黒画像であれば、入力ニューロンの数は4096 = 64 × 64 になり、色の度合いはメイドを0から1の適切な値で表す. 出力層は1つのニューロンからなり、出力値が0.5以上なら\"入力画像は9である\"ことを示し、0.5以下なら\"入力画像は9ではない”ということを示す.\n",
    "\n",
    "入出力層の設計が単純なのに対し、隠れ層の設計はかなり創造的なものになりうる.\n",
    "\n",
    "これから、ある層の出力が次の層の入力になるようなニューラルネットワークについて考察してみる. このようなネットワークはフィードフォーワードニューラルネットワーク(feedforward neural networks)と呼ばれる. これはネットワーク内にループがないということを意味している. しかし、フィードバックループを用いることが可能な、人工ニューラルネットワークモデル、再帰型ニューラルネットワーク(recurrent neural networks)と呼ばれる. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手書き数字を分類する単純なネットワーク\n",
    "------------------------\n",
    "私たちは手書き数字認識の問題を２つの問題に分けることができる. １つは、複数桁の数字からなる画像を、それぞれの数字からなる画像の列にすること. 私たち人間はこの分割問題(segmentation problem)を容易に解くことができるが、コンピュータプログラムが正確に画像を分類することは容易ではない."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
