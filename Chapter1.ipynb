{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "--------------\n",
    "\n",
    "### ニューラルネットとは\n",
    "--------\n",
    "コンピュータに、観測データにもとづいて学習する能力を与える、生物学にヒントを得たプログラミングパラダイム。\n",
    "\n",
    "### 深層学習とは\n",
    "------\n",
    "ニューラルネットワークに学習をさせるための強力な手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章\n",
    "\n",
    "------------------------\n",
    "\n",
    "## パーセプトロンとは\n",
    "\n",
    "- 人工ニューロンの1種\n",
    "- 1950年代から1960年代にかけて開発される\n",
    "- 今日では、パーセプトロン以外の人工ニューロンモデルを扱うことが一般的\n",
    "- 現代のニューラルネットワークの研究の多くは、シグモイドニューロンと呼ばれるモデルが主に使われている\n",
    "\n",
    "### パーセプトロンの仕組みについて\n",
    "\n",
    "![title](image/perceptron.png)\n",
    "\n",
    "- 複数の2進数x1,x2,...を入力にとり、1つの2進数を出力する\n",
    "- それぞれの入力が出力に及ぼす影響の大きさを表す実数\"重み\"(w1,w2,...)を用いて出力を計算\n",
    "- パーセプトロンの出力が0になるか1になるかは、入力の重みつき和$\\sum_j w_j x_j$と閾値の大小比較で決まる\n",
    "- 重みと同じく、閾値もパーセプトロンの挙動を決める実数パラメータ\n",
    "\n",
    "\\begin{equation}\n",
    "    output = \\left\\{ \\begin{array}{ll}\n",
    "        0 \\hspace{15pt}if \\sum_j w_j x_j \\leq threshold \\\\\n",
    "        1 \\hspace{15pt}if \\sum_j w_j x_j > threshold\n",
    "    \\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "- パーセプトロンを複雑に組み合わせることで、複雑な判断も可能になる。\n",
    "\n",
    "--------------------------------\n",
    "ここで、先程の条件式は煩雑なので書き換えてみる. \n",
    "\n",
    "まず、$\\sum_j w_j x_j $ という和は内積を使って、$ w \\cdot x \\equiv \\sum_j w_j x_j $ と書くことができる.  \n",
    "次に、閾値を不等式の左辺に移項し、パーセプトロンのバイアス $b \\equiv -threshold $ と呼ばれる量に置き換える.  \n",
    "すると式は次のようになる.\n",
    "\n",
    "\\begin{equation}\n",
    "    output = \\left\\{ \\begin{array}{ll}\n",
    "        0 \\hspace{15pt}if \\ w \\cdot x + b \\leq 0  \\\\\n",
    "        1 \\hspace{15pt}if \\ w \\cdot x + b  >  0\n",
    "    \\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "バイアスは、パーセプトロンが1を出力する傾向の高さを表す量だとみなすことができる.\n",
    "\n",
    "-------------------\n",
    "ここまでは、パーセプトロンを入力情報に重みをつけて判断を行う手続きとして用いてきたが、論理関数を計算するという別の用途もある.\n",
    "\n",
    "あらゆる計算は、AND, OR, そしてNANDから構成されている、とみなすことができる.パーセプトロンは、こういった論理関数を表現できる.例えば、２つの入力をとり、どちらも重みが-2で、全体のバイアスが3であるようなパーセプトロンを考えてみる.\n",
    "\n",
    "![title](image/nand.png)\n",
    "\n",
    "すると、このパーセプトロンは00を入力されると、$(-2) * 0 + (-2) * 0 + 3 = 3$ は正の数だからである.同じように計算すると、01や10を入力しても1を出力することがわかるが、11を入力した場合だけ0が出力される.ということは、このパーセプトロンは、NANDゲートを実装していることになる.\n",
    "\n",
    "よって、NANDゲートがあればどんな計算も構成することができるので、パーセプトロンのネットワークさえあれば任意の論理関数を計算することができる.\n",
    "\n",
    "以上より、パーセプトロンはどの計算装置にも負けない強力さを持つが、単にNANDゲートの亜種にすぎないとも言える.  ところが、ニューラルネットワークの重みとバイアスを自動的に最適化するような、学習アルゴリズム を開発することができるからである.この最適化は、プログラマの直接介入なしに、外部刺激に反応して勝手に起こる.これのおかげで、人工ニューロンは、従来の理論ゲートとは全く異なった使い方ができるようになった.NANDゲートや他の種類の論理ゲートはすべて手動で配線してやる必要があったのに対し、ニューラルネットワークは問題の解き方を自発的に学習してくれる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## シグモイドニューロン\n",
    "--------------------\n",
    "\n",
    "### ニューラルネットワークに対してアルゴリズムをどう設計するか\n",
    "\n",
    "もし、重みやバイアスを微笑に変化させた場合の出力の変化もまた微小である、という性質が本当に成り立っていれば、その声質を使って、ニューラルネットワークがより自分の思ったとおりの挙動を示すように重みとバイアスを修正できる.\n",
    "\n",
    "![title](image/smallchange.png)\n",
    "\n",
    "「９」であるべき数字を間違って「８」と分類したときに、私たちは重みやバイアスに変化を与えて、どうすればこのニューラルネットワークがこの画像を正しく「９」と分類する方向に近づくかを探ることができる.  \n",
    "この過程を繰り返し、重みとバイアスを変化させ続ければ、生成される結果は次第に改善されていく. このようニューラルネットワークは学習する.\n",
    "\n",
    "問題は、ニューラルネットワークがパーセプトロンで構成されていたとすると、このような学習は怒らないこと. 実際、ニューラルネットワーク内のパーセプトロンのうち、どれか１つの重みやバイアスを少し変えると、そのパーセプトロンの出力は、変化がないか、もしくは0から1へというようにすっかり反転してしまう.このように反転してしまうと、ニューラルネットワークの他の部分の挙動も、連動して複雑に変わっていってしまう.\n",
    "\n",
    "今のところパーセプトロンで構成されたニューラルネットワークに上手に学習させる方法は明らかにはなっていない.\n",
    "\n",
    "### シグモイドニューロンとは\n",
    "\n",
    "前述のような問題は、シグモイドニューロンと呼ばれる人工ニューロンを導入することによって克服することができる.  シグモイドニューロンはパーセプトロンと似ているが、シグモイドニューロンの重みやバイアスに微小な変化を与えたとき、それに応じて生じる出力の変化も微小なものにとどまるように調整されている.これによって、シグモイドニューロンで構成されているニューラルネットワークは学習可能になる.\n",
    "\n",
    "![title](image/perceptron.png)\n",
    "\n",
    "パーセプトロンと同じように、シグモイドニューロンは$x_1, x_2,...$ といった入力をとる.しかし、単に0や1だけでなく、0から1の間のあらゆる値をとることができる.また、重みやバイアスもパーセプトロンと同じように持っている. しかし、出力は、０や１だけではなく、代わりに$ \\sigma(w \\cdot x + b) $ という値をとる. $\\sigma$はシグモイド関数と呼ばれていて、次の式で定義される:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\sigma(z) \\ \\equiv \\frac{1}{1+e^{-z}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "より明確に表現すると、シグモイドニューロンの出力は、入力が$x_1, x_2,...$ で、重みが$w_1, w_2, ...$で、そしてバイアスが$b$のとき、次の形をとる.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\frac{1}{1 \\ + \\ \\exp(- \\ \\Sigma_j w_j x_j \\ - \\ b)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "一見、パーセプトロンとシグモイドニューロンは大きく異るように見えるが、大きく共通点がある.\n",
    "\n",
    "パーセプトロンとの共通点を理解するために、$z \\equiv w \\cdot x + b$を大きな正の数としてみる. このとき、$e^{-z} \\approx 0 $ つまり $\\sigma(z) \\approx 1$となる. 言い換えると、$ z=w \\cdot x + b $が大きな数であるとき、シグモイドニューロンの出力はほぼ1となり、パーセプトロンと同じになる. 逆に、$ z = w \\cdot x + b$を大きな負の数としてみる. そのとき$ e^{-z} \\to \\infty $ であり、$\\sigma(z) \\approx 0 $ となる. つまり、$ z=w \\cdot x + b $が大きな負の数であるときも、シグモイドニューロンはパーセプトロンとほぼ同じ動きをする. ただし、$ w \\cdot x + b $ がそこまで大きな数でない場合は、パーセプトロンと同じにはならない.\n",
    "\n",
    "$\\sigma$について重要なのはどういう形のグラフであるかである.数がそのグラフの形である.\n",
    "\n",
    "![title](image/sigmoidgraph.jpg)\n",
    "\n",
    "このグラフはステップ関数のなめらか版である.\n",
    "\n",
    "![title](image/stepgraph.jpg)\n",
    "\n",
    "もし$\\sigma$がステップ関数であれば、シグモイドニューロンはパーセプトロンと等しくなる. $\\sigma$関数の滑らかさは、重みについて$\\Delta w_j$、バイアスについて$\\Delta b$の小さな変化は、ニューロンの出力について$\\Delta \\mbox{output}$の小さな変化を生み出すということを意味している.実際下記の計算から、$\\Delta \\mbox{output}$は大体うまくいってることがわかる.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\Delta \\mbox{output} \\approx \\Sigma_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "    \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\Delta \\mbox{output}$は重みとバイアスにおいて、$\\Delta w_j$ と $\\Delta b$の変化に対し、線形である、といっているのである. この線形性は、欲しいoutputがどんな小さな変化でも、重みとバイアスを小さく変化させることで簡単に得られることを示している. このことからシグモイドニューロンが、より容易に重みとバイアスの変化がoutputを変化させることがわかる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークのアーキテクチャ\n",
    "------------------\n",
    "手書き数字の分類においてとても上手く働くニューラルネットワークを紹介する.その準備として、いくつかの専門用語を説明するためにニューラルネットワークのそれぞれの部門に名前をつけておく.\n",
    "\n",
    "![title](image/neuralnetwork.png)\n",
    "\n",
    "一番左の層が入力層(input layer)、真ん中のニューロンを入力ニューロン(input neurons)、一番右の層または出力層(output layer)は、出力ニューロン(ouput neuranos)から構成されている. 中央の層は、隠れ層(hidden layer)と呼ばれる. 隠れ層が複数あるような複数層のネットワークをときおり多層パーセプトロン(multilayer perceptrons)、またはMLPsと呼ぶ. しかし、実際はパーセプトロンではなく、シグモイドニューロンである.\n",
    "\n",
    "例えば、手書きの画像が9かそうでないかを判定したいとする. もしその画像が64×64の白黒画像であれば、入力ニューロンの数は4096 = 64 × 64 になり、色の度合いはメイドを0から1の適切な値で表す. 出力層は1つのニューロンからなり、出力値が0.5以上なら\"入力画像は9である\"ことを示し、0.5以下なら\"入力画像は9ではない”ということを示す.\n",
    "\n",
    "入出力層の設計が単純なのに対し、隠れ層の設計はかなり創造的なものになりうる.\n",
    "\n",
    "これから、ある層の出力が次の層の入力になるようなニューラルネットワークについて考察してみる. このようなネットワークはフィードフォーワードニューラルネットワーク(feedforward neural networks)と呼ばれる. これはネットワーク内にループがないということを意味している. しかし、フィードバックループを用いることが可能な、人工ニューラルネットワークモデル、再帰型ニューラルネットワーク(recurrent neural networks)と呼ばれる. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 手書き数字を分類する単純なネットワーク\n",
    "------------------------\n",
    "私たちは手書き数字認識の問題を２つの問題に分けることができる. １つは、複数桁の数字からなる画像を、それぞれの数字からなる画像の列にすること. 私たち人間はこの分割問題(segmentation problem)を容易に解くことができるが、コンピュータプログラムが正確に画像を分類することは容易ではない. そして、2つめの問題は、数字を分類すること. これから個々の手書き数字の認識問題を解くニューラルネットワークを開発していく.\n",
    "\n",
    "それぞれの数字を認識するために、3層のニューラルネットワークを用いる.\n",
    "\n",
    "![title](image/3neuralnet.png)\n",
    "\n",
    "ネットワークの入力層はピクセルの値をエンコードするニューロンを持っている. 2番目の層は隠れ層で、この隠れ層のニューロンの数をnとし、nの値を変えて実験する. 出力層は１０ニューロンから構成されている. もし最初のニューロンが発火（出力 \\approx 1)したら、それは、ネットワークがその数字を０だと思っていることを示す. \n",
    "\n",
    "このネットワークのゴールは、入力の画像に対してそれがどの数字なのか示すことである. これを行うのに自然だと思われる方法は、4つの出力ニューロンを用いて、それぞれのニューロンを用いて、それぞれのニューロンで出力が0または1に近いかに応じてバイナリの値をとることである. 答えをエンコードするには4つのニューロンで十分である. なぜなら$ 2^4 = 16 $は入力の数字の10より多くの値をとることができるから. ではなぜ、その代わりに10個のニューロンを用いるのだろうか. これは経験にもとづくものであり、10個の出力ニューロンをもったネットワークの方がうまくがくしゅうすることがわかったのである.\n",
    "\n",
    "なぜこうするか理解するために、ニューラルネットワークが何をしているか、その原理から考えてみる. 10個の出力ニューロンを用いた場合についてまず考察してみると、最初の出力ニューロンに焦点を当ててみると、これはその数字が０かどうか決めようとしていることがわかる. これは隠れ層からの乗法を考量して行う. 隠れそうは何をしているか. 議論のためにとりあえず、隠れそうの最初のニューロンが、下記のような画像が存在するかどうかを検出すると考えてみる.\n",
    "\n",
    "![title](image/zeroleftup.png)\n",
    "\n",
    "その検出は、その画像と重なった入力ピクセルに重く重み付けし、他の入力には軽い重み付けをすることで、行うことができる. 同様の方法で、2番目、３番目、４番目の隠れニューロンも、下記のような画像が存在するかどうかを検出すると考えてみる.\n",
    "\n",
    "![title](image/3zeros.png)\n",
    "\n",
    "これらの4つの画像は、合わせると、数字の列の0の画像になる. つまり、もしその4つ全ての隠れニューロンが発火したら、私たちはその数字が０であると結論付けることができる. もちろんそれだけがその画像が０であると結論づく証拠ではなく、その他の多くの方法(例えば上記の画像の変換や僅かな歪みによって）合理的に０を得ることができる. しかし少なくともこの場合では入力は0だと結論づけて差し支えないだろう.\n",
    "\n",
    "ニューラルネットワークがこの方法で機能するとすれば、なぜ10出力ニューロンの方がいいのかについて尤もな説明を得ることができる. もし4つの出力だとすると、最初の出力ニューロンは何がその数字の最も重要な部分なのか決めようとするだろう. しかしその最も重要な部位を、上に示したような単純な形状に関連付ける方法はない. おそらく4出力ニューロンのネットワークであろうと、巧妙な学習アルゴリズムがうまく重みの割当てを見つけるだろうが、経験則的に10出力ニューロンのニューラルネットワークはうまくいくことがわかっている."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 勾配降下法を用いた学習\n",
    "---------\n",
    "数字の認識を行うために、数万件の手書き数字スキャン画像とその正しい分類からなるMNISTデータセットを用いる.MNISTは２つの要素からなっていて、１つめは60000個の訓練用の画像で250人(半数はCensus BUreauの従業員で、残り半数は高校生）の手書きの標本からスキャンされたものである.これらの画像は２８×２８ピクセルのグレースケールとなっている。２つめは10000個のテスト用画像で、同様に28×28ピクセルのグレースケールとなっている.これらを用いて評価する.テストの精度を良くするため、テストデータは訓練用データとは異なる250人から採取されている.\n",
    "\n",
    "ここで訓練入力を$x$と定義する.これで書く入力$28×28 = 784$１次元ベクトルを$x$とみなせて好都合である.また出力を$　y=y(x)　$と定義し、この$y$を10次元のベクトルとする.私達がえたいものは、全訓練入力xについて、ネットワークの出力が$y(x)$になるべく近くなるような重みとバイアスを見つけるアルゴリズムである.この目標をどれだけ達成できたかを測るためコスト関数を定義する:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    C(w,b) \\equiv \\frac{1}{2n} \\sum_x \\|y(x) - a\\|^2.\n",
    "\\tag{6}\\end{eqnarray}\n",
    "\n",
    "ここで$w$はネットワーク中の全ての重み、$b$は全バイアス、nは訓練入力の総数、$a$は入力が$x$のときにネットワークから出力されるベクトルである.出力$a$は＄ｗ＄と＄ｂ＄そして$x$に依存するが、表記をシンプルにするためにここでは明示していない.$\\|v\\|$はベクトル$v$の距離関数を示す記号である.Cは2次コスト関数と呼ぶ。これはしばしば平均二乗誤差あるいは単にMSE(mean squared error)としても知られている.2次コスト関数の式を見てみると総和の全項目が非負なので、$C(w,b)$は非負になることがわかる.訓練アルゴリズムの狙いは重みとバイアスの関数$C(w,b)$の最小化である.言い換えれば可能な限りコストを小さくできる重みとバイアルの組を見つけることである.それを勾配降下法というアルゴリズムを使って行う.\n",
    "\n",
    "２次コストを導出する理由は、重みとバイアスに対してどう変化を加えればコストを改善できるのかが簡単にわかるようになるからである.2次コストの最小化をした後ではじめて分類の精度を調べることにする.実際２次関数が滑らかであるとしても、アドホックな選択である。仮に違うコスト関数を選んだ場合、最小化する重みとバイアスの組は全く異なってくるのかもしれなく、後に私たちはコスト関数をもっかい考えいくつかの修正を行うことになる.\n",
    "\n",
    "私達のゴールである２次コスト関数$C(w,b)$を最小化する重みとバイアスを見つけるという問題は、いまの定式化おままではたくさんの注意をそらすポイントをもっている.それは、重みとバイアスの解釈、背後に潜んでいるシグモイド関数、ネットワーク構造の選択、MNIST等がある.しかし、今は関数を最小化することだけを考える.後に、最小化したいニューラルネットワークの具体的な関数に戻ってくるとする.\n",
    "\n",
    "私たちは$C(v)$を最小化しようとしているとする. $C(v)$は複数の引数$ v = v_1, v_2, ... $ を取って、実数の値を返す関数であればなんでもかまわない. ここで$C(v)$を最小化するのに$C$が２つの変数から関数だと考えることにする.\n",
    "\n",
    "![title](image/gradient.png)\n",
    "\n",
    "この問題の１つの攻略法は、微積分を使って解析的に最小値を見つけることである.しかし、ニューラルネットワークの場合、１０億の重みとバイアスを持っており極めて複雑になるため、微積分による最小化は機能しない. しかし、上のグラフの上をボールが転がる動きをシュミレートする考えを用いることができる. おそらく単にCの導関数を微分すればこのシュミレーションが行えるだろう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
